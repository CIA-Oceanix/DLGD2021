{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryjvDoXuzoXx"
   },
   "source": [
    "# Study of Burgers's equation in 1D\n",
    "\n",
    "## Physical model\n",
    "As physical model we use Burgers equation.\n",
    "This equation is non-linear and can lead to interesting shock formations. Hence, it's a very good starting point for experiments, and it's 1D version is given by:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial u}{\\partial{t}} + u \\nabla u =\n",
    "    \\nu \\nabla\\cdot \\nabla u\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhYaNUrr0XD-"
   },
   "source": [
    "Preliminaries: first we import the phiflow library to generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkSzEv4LHt3Z"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet phiflow  # Install stable release\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIlyCgpl9vew"
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "from phi.torch.flow import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3_1keg60oxB"
   },
   "source": [
    "Let define and initialize the necessary constants:\n",
    "our simulation domain will have `N=128` cells as discretization points for the 1D velocity $u$ in a periodic domain $\\Omega$ for the interval $[-1,1]$, using 32 time `STEPS` for a time interval of 1, giving us `DT=1/32`. Additionally,  viscosity `NU` is set to $\\nu=0.01/\\pi$.\n",
    "\n",
    "The initial state is given by $-\\text{sin}(\\pi x)$ in the numpy array `INITIAL_NUMPY`, which will be used to initialize the velocity $u$ in the simulation in the next cell. This initialization will produce a nice shock in the center of the domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97YADT3mHwiP"
   },
   "outputs": [],
   "source": [
    "N = 128\n",
    "STEPS = 32\n",
    "DT = 1./STEPS\n",
    "NU = 0.01/np.pi\n",
    "\n",
    "# initialization of velocities\n",
    "INITIAL_NUMPY = np.asarray( [-np.sin(np.pi * x) * 1. for x in np.linspace(-1,1,N)] ) # 1D numpy array\n",
    "INITIAL = math.tensor(INITIAL_NUMPY, spatial('x') ) # convert to phiflow tensor\n",
    "velocity = CenteredGrid(INITIAL, extrapolation.PERIODIC, x=N, bounds=Box[-1:1])\n",
    "velocities = [velocity]\n",
    "age = 0.\n",
    "for i in range(STEPS):\n",
    "    v1 = diffuse.explicit(velocities[-1], NU, DT)\n",
    "    v2 = advect.semi_lagrangian(v1, v1, DT)\n",
    "    age += DT\n",
    "    velocities.append(v2)\n",
    "# get \"velocity.values\" from each phiflow state with a channel dimensions, i.e. \"vector\"\n",
    "vels = [v.values.numpy('x,vector') for v in velocities] # gives a list of 2D arrays "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilnrZRdH1WfE"
   },
   "source": [
    "Once the simulation is performed, let's convert the data into PyTorch tensors. $x$ is the spatial domain, $t$ is the time, $y$ is the velocity computed from the simulation. In a supervised learning framework, we need triplet values $(x_i, t_i, y_i)$ where $i$ denotes a sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "1vhLqHvATBp2",
    "outputId": "18211489-0ef2-4bf7-d638-c50ec2531a40"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vels = [v.values.numpy('x,vector') for v in velocities] # gives a list of 2D arrays \n",
    "x = torch.reshape(torch.linspace(-1,1,128),(-1,1))\n",
    "t = torch.reshape(torch.linspace(0,1,33),(-1,1))\n",
    "x = x.repeat(33,1)\n",
    "t = t.repeat_interleave(128, dim=0)\n",
    "v = np.array(vels).reshape(-1,1)\n",
    "y = torch.from_numpy(v)  \n",
    "\n",
    "def plot_1d(y_vis):\n",
    "  fig = pylab.figure().gca()\n",
    "  fig.plot(y_vis[0,:], color='blue',  label=\"t=0\")\n",
    "  fig.plot(y_vis[10,:], color='green', label=\"t=0.3125\")\n",
    "  fig.plot(y_vis[20,:], color='cyan',  label=\"t=0.625\")\n",
    "  fig.plot(y_vis[32,:], color='purple',label=\"t=1\")\n",
    "  pylab.xlabel('x'); pylab.ylabel('u'); pylab.legend()  \n",
    "\n",
    "plot_1d(y.reshape((33,128)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o2zYFoJ2W00"
   },
   "source": [
    "The 1D display shows the shock developing in the center of the domain, which forms from the collision of the two initial velocity \"bumps\", the positive one on left (moving right) and the negative one right of the center (moving left).\n",
    "\n",
    "An alternative approach to visualize the velocities is by displaying an image (the vertical axis is the spatial domain while the other axis is the \"dilated\" time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "8vhU2TsfBAPi",
    "outputId": "9b5a26db-40c9-460c-c63b-54285f341278"
   },
   "outputs": [],
   "source": [
    "def plot_2d(y_vis):\n",
    "  fig, axes = pylab.subplots(1, 1, figsize=(16, 5))\n",
    "  im = axes.imshow(y_vis, origin='upper', cmap='inferno')\n",
    "  pylab.colorbar(im) ; pylab.xlabel('time'); pylab.ylabel('x'); \n",
    "\n",
    "plot_2d(np.transpose(y.reshape(33,128)).repeat_interleave(16, dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2j013nb3GOc"
   },
   "source": [
    "## Supervised learning\n",
    "Let's consider a reconstruction (or interpolation) task:\n",
    "$\n",
    "\\text{arg min}_{\\theta} \\sum_i ( f(x_i ; \\theta)-y_i )^2 \n",
    "$\n",
    "where $x$ and $y$ are solutions of $u$ at different locations in space and time. Note that there is no constraint for the boundary conditions.\n",
    "\n",
    "First, set up a neural network that will be used to approximate the velocities, define appropriate loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ek7r2NZPR_C"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class burgers_net(nn.Module):\n",
    "    \n",
    "    # class initialization\n",
    "    def __init__(self, input_size, hidden_size, n_layers, output_size):\n",
    "        super(burgers_net, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.fc0 = nn.Linear(input_size, hidden_size)\n",
    "        self.fcs = torch.nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "          self.fcs.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.fcn = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    # function to apply the neural network\n",
    "    def forward(self, x, t):\n",
    "        xt = torch.cat((x,t),dim=1)\n",
    "        y = self.fc0(xt)\n",
    "        y = nn.Tanh()(y)\n",
    "        for i in range(self.n_layers):\n",
    "          y = self.fcs[i](y)\n",
    "          y = nn.Tanh()(y)\n",
    "        y_pred = self.fcn(y)  \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UaTFaoLSbDB"
   },
   "outputs": [],
   "source": [
    "net_model = burgers_net(input_size = 2,hidden_size = 20, n_layers = 2, output_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxYXybJDSwRA"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "mse = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net_model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-rdhPAd4Gen"
   },
   "source": [
    "Start the training step ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "7VPPTxzedV0P",
    "outputId": "098816ad-a2ff-45e9-873d-fd9eacc2d3b3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "epochs = 10000 # number of epochs\n",
    "losses = [] # list to stock the loss at each iteration\n",
    "\n",
    "# Loop on epochs\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # compute the prediction using the previous parameters of the neural network\n",
    "    y_pred = net_model.forward(x,t)\n",
    "    \n",
    "    # compute and stock the loss\n",
    "    loss = mse(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # initialize the gradient to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # compute the gradient by back propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the parameter values using the gradient\n",
    "    optimizer.step()\n",
    "\n",
    "plot(range(epochs), losses)\n",
    "title('Loss function', size=20)\n",
    "xlabel('Epoch', size=20)\n",
    "ylabel('Loss value', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "XUpYcheIeWQE",
    "outputId": "d9c3535e-8313-449f-e3b8-ba706e1dcb41"
   },
   "outputs": [],
   "source": [
    "# plot the adjustment at the last epoch\n",
    "plot(x[0:128,0], y[0:128,0], 'r.')\n",
    "plot(x[0:128,0], y_pred.detach().numpy()[0:128,0], 'g.')\n",
    "legend(['Simulation', 'Fit'], prop={'size': 20})\n",
    "title('Nonlinear regression', size=20)\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G89og_8D8xRK"
   },
   "source": [
    "We can plot the estimated velocities using the ```plot_1d``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "a32r-TZoe9w0",
    "outputId": "b2ed3ef6-d1ee-48f9-97d7-34e0d3fe11e0"
   },
   "outputs": [],
   "source": [
    "y_vis = y_pred.reshape((33,128)).detach().numpy()\n",
    "plot_1d(y_vis)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4hgBUks85N3"
   },
   "source": [
    "We can also plot the error between the estimated velocities and the simulated values using the ```plot_1d``` function. For quantitative purpose, we can compute the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "sufdY-PXe2YA",
    "outputId": "38c9346d-224c-4524-916e-440f66bc8ca8"
   },
   "outputs": [],
   "source": [
    "y_true = y.reshape((33,128))\n",
    "\n",
    "plot_1d(y_true - y_vis)\n",
    "\n",
    "print(np.mean(np.abs(y_true.numpy()-y_vis).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "F6USZvivhx2K",
    "outputId": "3ecda4e8-7917-4466-e493-5416ff3ff04f"
   },
   "outputs": [],
   "source": [
    "fig = pylab.figure().gca()\n",
    "fig.plot(y_vis[32,:], color='purple',label=\"t=1\")\n",
    "fig.plot(y_true[32,:], color='blue',label=\"t=1\")\n",
    "pylab.xlabel('x'); pylab.ylabel('u'); pylab.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "BlBdVG62dgmU",
    "outputId": "1b660341-6ac9-44c7-f285-40e27e694864"
   },
   "outputs": [],
   "source": [
    "y_vis = torch.transpose(y_pred.reshape(33,128),0,1).repeat_interleave(16, dim=1).detach().numpy()\n",
    "plot_2d(y_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_miy_KAagIwH"
   },
   "source": [
    "## Subsampling\n",
    "\n",
    "Let's consider now a similar supervised learning problem with subsampled data. We only keep 200 points for the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "OytA9H2W81uz",
    "outputId": "e68839c4-df33-4a77-da9d-0bc673f4fe91"
   },
   "outputs": [],
   "source": [
    "idx = torch.randperm(x.shape[0])\n",
    "n_samples = 200\n",
    "subx = x[idx[0:n_samples],:]\n",
    "subt = t[idx[0:n_samples],:]\n",
    "suby = y[idx[0:n_samples],:]\n",
    "\n",
    "net_model_2 = burgers_net(input_size = 2,hidden_size = 20, n_layers = 2, output_size = 1)\n",
    "optimizer = torch.optim.Adam(net_model_2.parameters(), lr = 0.0001)\n",
    "\n",
    "epochs = 10000 # number of epochs\n",
    "# Loop on epochs\n",
    "for i in range(epochs):\n",
    "    y_pred = net_model_2.forward(subx,subt)\n",
    "    loss = mse(y_pred, suby)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "y_pred = net_model_2.forward(x,t)\n",
    "y_vis = y_pred.reshape((33,128)).detach().numpy()\n",
    "\n",
    "plot_1d(y_vis)\n",
    "\n",
    "print(np.mean(np.abs(y_true.numpy()-y_vis).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b149j_jT5Odz"
   },
   "source": [
    "## Physically informed deep learning\n",
    "\n",
    "We consider now the following reconstruction problem:\n",
    "\n",
    "$\n",
    "\\text{arg min}_{\\theta} \\sum_i ( f(x_i ; \\theta)-y_i )^2 + R(f(x_i ; \\theta)) ,\n",
    "$\n",
    "\n",
    "The residual function $R$ collects additional evaluations of $f(;\\theta)$ and its derivatives to formulate the residual of the physical modeling. This approach -- using derivatives of a neural network to compute a PDE residual -- is typically called a _physics-informed_ approach, yielding a _physics-informed neural network_ (PINN) to represent a solution for the inverse reconstruction problem.\n",
    "\n",
    "In the formulation above, $R$ should simply converge to zero above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoDF1is67ueQ"
   },
   "source": [
    "Start to write the function for the computation of $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVMcwZB0kOjm"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############\n",
    "\n",
    "def residual_function(u,x,t):\n",
    "\n",
    "    ...\n",
    "    \n",
    "    return u_t + u*u_x - (0.01 / np.pi) * u_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OAzb2t0XgOI1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7immiqkm72y-"
   },
   "source": [
    "Perform the training using the new losses (data-fidelity and regularization terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "JPga-Ivxm_Ek",
    "outputId": "01496e38-e6b5-432b-b556-e0ddda9ff324"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############\n",
    "\n",
    "...\n",
    "\n",
    "# Loop on epochs\n",
    "for i in range(epochs):\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PJ6KK8A8Qws"
   },
   "source": [
    "Plot the estimated velocities, as well as the error between the predicted values and the simulated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "Th8RL6shF7xT",
    "outputId": "b0d6d535-19f6-4a00-b52a-1eab818836df"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "HUlC4J7jcndk",
    "outputId": "ff6107de-43d2-430c-c6a0-85d1e7e26355"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "cVd4km2KLLHC",
    "outputId": "2f027c00-fc44-4cc3-f543-cfa982aed83c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Burgers_1D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
