{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9x2xrAY8O5OM"
   },
   "source": [
    "# Introduction to automatic differentiation using PyTorch\n",
    "\n",
    "A neural network is based on the minimization of a cost function (also called loss function) w.r.t. to some parameters, these parameters corresponding to the weights of the network. The loss function measures the adequacy between the observed data and the model (i.e., the neural network).\n",
    "\n",
    "In deep learning architectures, loss functions are complicated, corresponding to a succession of several activation functions (e.g., linear, ReLU, sigmoid). Minimizing this function is thus tricky. The minimization procedure is performed using gradient descent algorithms, but computing gradients of complicated loss function is not straightforward.\n",
    "\n",
    "Deep learning libraries like PyTorch use automatic differentiation to compute efficiently and rapidly the gradient of the loss function at each epoch. The goal here is to implement a minimization problem using automatic differentiation and compare it with a classic method based on the exact calculation of the derivative. Then, the second objective will be to implement, by hand, the iterations of the optimization of a neural network.\n",
    "\n",
    "First, let's define a sigmoid function, well known is neural networks, especially for classification problems: tanh. The exact derivative of tanh is also defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tRfD7p2RLzr",
    "outputId": "252ca38b-2951-471f-ef38-dd84d2fb646f"
   },
   "outputs": [],
   "source": [
    "# Import classic libraries (Matplotlib and PyLab)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters (figure size and random seed)\n",
    "pylab.rcParams['figure.figsize'] = (15,15)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNrIr-DRO0ab"
   },
   "outputs": [],
   "source": [
    "# Define tanh function\n",
    "def f(x):\n",
    "    return (torch.tanh(x))\n",
    "\n",
    "# Define the first derivative of tanh\n",
    "def f_prime(x):\n",
    "    return (1 - torch.tanh(x)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpFMOEG9PIyk"
   },
   "source": [
    "Then, we compare the analytic solution to the approximation computed by automatic differentiation in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "Z7GrM6yXPLIH",
    "outputId": "e91950e7-6aa8-48dd-b51d-898c265a47a5"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = torch.linspace(-5.0, 5.0, requires_grad=True)\n",
    "f_x = f(x)\n",
    "f_prime_x = f_prime(x)\n",
    "\n",
    "# Apply the automatic differentiation\n",
    "y = torch.sum(f(x))\n",
    "\n",
    "grads = torch.autograd.grad(y, x, create_graph=True)[0] \n",
    "#Alternative to torch.autograd.grad:\n",
    "#y.backward(retain_graph=True)\n",
    "#x.grad\n",
    "\n",
    "\n",
    "# Plot results\n",
    "subplot(2,1,1)\n",
    "plot(x.detach().numpy(), f_x.detach().numpy(), 'b')\n",
    "plot(x.detach().numpy(), f_prime_x.detach().numpy(), 'g')\n",
    "#plot(x.detach().numpy(), x.grad.detach().numpy(), 'r--', linewidth=5)\n",
    "plot(x.detach().numpy(), grads.detach().numpy(), 'r--', linewidth=5)\n",
    "plot(array([-5, 5]), array([0, 0]), 'k--')\n",
    "legend(['f(x) = tanh(x)', 'f prime (analytic)', 'f prime (automatic)'], prop={'size': 20})\n",
    "title('Tanh and its first derivative', size=20)\n",
    "xlabel('x', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGWpdjhuPRHr"
   },
   "source": [
    "Note: you can use both ```torch.autograd.grad``` or ```y.backward()``` to compute the gradients of $y$ with respect to $x$.\n",
    "\n",
    "Let's use automatic differentation to estimate network parameters.\n",
    "\n",
    "We consider the same non-linear regression problem as previously: $y=2+0.5x-0.05x^2$ and generate $y$ using an additional Gaussian standard random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "yi-rbaVUPXdv",
    "outputId": "9d5a15d5-c976-4d80-9ae0-bd0b5bce6042"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = torch.randn(1000, 1)*10 # input variable\n",
    "y_true = 2 + 0.5*x - 0.05*x**2 # true model\n",
    "y = y_true + torch.randn(1000, 1)*2 # add noise to the truth\n",
    "\n",
    "# Plot noisy data and true model\n",
    "plot(x, y_true, 'r.')\n",
    "plot(x, y, 'b.')\n",
    "legend(['Model', 'Data'], prop={'size': 20})\n",
    "title('Nonlinear regression', size=20)\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDiPQ9MoP18k"
   },
   "source": [
    "Let's consider the same neural network and the MSE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3022qVIP87t"
   },
   "outputs": [],
   "source": [
    "# Declare a class for nonlinear regression\n",
    "class nonlinear_regression_nn(nn.Module):\n",
    "    \n",
    "    # class initialization\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(nonlinear_regression_nn, self).__init__()\n",
    "        # fully connected layer with linear activation\n",
    "        self.fc0 = nn.Linear(input_size, hidden_size)\n",
    "        # ReLu activation\n",
    "        self.relu = nn.ReLU()\n",
    "        # fully connected layer with linear activation\n",
    "        self.fc1 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    # function to apply the neural network\n",
    "    def forward(self, x):\n",
    "        out = self.fc0(x)\n",
    "        out = self.relu(out)\n",
    "        y_pred = self.fc1(out)\n",
    "        return y_pred\n",
    "    \n",
    "# Create the neural network (1 input size for x, 6 neurons in the hidden layer, and 1 output size for y)\n",
    "nonlinear_regression_model = nonlinear_regression_nn(1, 10, 1)\n",
    "\n",
    "# Loss function: MSE = sum [(y - y_pred)^2]\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpPr9MonQQQk"
   },
   "source": [
    "Write the code to perform the optimization using the automatic differentiation. Look at the convergence (evolution of the loss) and compare the estimated model with the true model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "_DdwrFatQUu8",
    "outputId": "b4874766-f83e-4bf2-872a-e65b02da7202"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "wY-B7LynRcaj",
    "outputId": "2d5fbc8e-cd82-4dda-a4ed-dffd13693198"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0hrb9TFTEX8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Automatic_differentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
