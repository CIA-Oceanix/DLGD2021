{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT0MDDhwIhwl"
   },
   "source": [
    "# Introduction to regression using PyTorch\n",
    "\n",
    "The aim of this session is to explain $y$ (output variable) as a function of one or several $x$ (input variables) using neural networks. Here, $x$ and $y$ are continuous (regression problem).\n",
    "\n",
    "We use the PyTorch library to implement deep learning architectures. This interface has the following characteristics:\n",
    "- it is using tensors, kind of arrays that can be easily manipulated by GPUs\n",
    "- it is using the automatic differentiation to easily compute the gradient of complex functions\n",
    "- it is one of the most important deep learning library\n",
    "- it can interact with classical machine learning libraries like *Scikit-learn*\n",
    "\n",
    "This lab session is an introduction. It gives you the general concepts of neural nets and helps you to implement them using a dedicated Python library. If you want to play with similar regression and classification examples in small dimensions, you can use http://playground.tensorflow.org/. For more details concerning the implementation of PyTorch, please visit this tutorial at https://pytorch.org/tutorials/.\n",
    "\n",
    "Authors: [pierre.tandeo@imt-atlantique.fr](pierre.tandeo@imt-atlantique.fr), [lucas.drumetz@imt-atlantique.fr](lucas.drumetz@imt-atlantique.fr), [simon.benaichouche@imt-atlantique.fr](simon.benaichouche@imt-atlantique.fr), [aurelien.colin@imt-atlantique.fr](aurelien.colin@imt-atlantique.fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkojd73gIeBQ",
    "outputId": "97c7522f-1124-415d-ee88-3e34cb56011b"
   },
   "outputs": [],
   "source": [
    "# Import classic libraries (Matplotlib and PyLab)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Parameters (figure size and random seed)\n",
    "pylab.rcParams['figure.figsize'] = (15,15)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FLsBvnWJG2g"
   },
   "source": [
    "**Linear regression**\n",
    "\n",
    "Simple regression is a linear problem between continuous variables $x$ and $y$. Here, we write the model $y=2+0.5x$ and generate $y$ using an additional Gaussian standard random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "g0X4vMEEJGLa",
    "outputId": "04029a0c-11f9-455f-e215-5dc1b31d0ca0"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = torch.randn(1000, 1)*10 # input variable\n",
    "y_true = 2 + 0.5*x # true model\n",
    "y = y_true + torch.randn(1000, 1)*2 # add noise to the truth\n",
    "\n",
    "# Plot noisy data and true model\n",
    "plot(x, y_true, 'r')\n",
    "plot(x, y, 'b.')\n",
    "legend(['Model', 'Data'], prop={'size': 20})\n",
    "title('Linear regression', size=20)\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VVBHqG6JRYo"
   },
   "source": [
    "The objective is to build a neural net to fit the relationship between $x$ and $y$. In PyTorch, it is necessary to create a specific class for each neural network architecture. Below, we declare the neural network corresponding to a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tS7A9yQMJM9Z"
   },
   "outputs": [],
   "source": [
    "# Declare a class for linear regression\n",
    "class linear_regression_nn(nn.Module):\n",
    "    \n",
    "    # class initialization\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(linear_regression_nn, self).__init__()\n",
    "        # fully connected layer with linear activation\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    # function to apply the neural network\n",
    "    def forward(self, x):\n",
    "        y_pred = self.fc(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpaFHUnqJXcl"
   },
   "source": [
    "Then, we create a neural network based on the specific architecture declared above. We finally check the values of the parameters (randomly generated).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5aBLsntJZjR",
    "outputId": "7b52041d-ae6a-4fd5-b567-73f3e578375a"
   },
   "outputs": [],
   "source": [
    "# Create the neural network (1 input size for x and 1 output size for y, bias is declared by default)\n",
    "linear_regression_model = linear_regression_nn(1, 1)\n",
    "\n",
    "# Print the model architecture\n",
    "print(linear_regression_model)\n",
    "\n",
    "# Print the model parameters (weights of the neural network)\n",
    "slope, bias = linear_regression_model.parameters()\n",
    "print('Slope: ', slope)\n",
    "print('Bias: ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FcVkC_aJfQk"
   },
   "source": [
    "In order to quantify the quality of the fit given by the neural network, we define a loss function. For a regression problem, the classic loss function is the Mean Squared Error (MSE). But other loss functions are already defined in PyTorch: https://pytorch.org/docs/stable/nn.html#loss-functions.\n",
    "\n",
    "Then, to compute the gradient of the loss function w.r.t. the parameters of the neural network, we use an optimization technique. A classic procedure is the Stochastic Gradient Descent (SQG). Other optimizers are coded in PyTorch: https://pytorch.org/docs/stable/optim.html#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6TEy5rQJhr5"
   },
   "outputs": [],
   "source": [
    "# Loss function: MSE = sum [(y - y_pred)^2], with y_pred = w0 + w1*x\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: new_parameters = old_parameters - lr*gradient, with lr the learning rate\n",
    "optimizer = torch.optim.SGD(linear_regression_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCfrYF2FJj1L"
   },
   "source": [
    "Now, we fit the neural network using the ($x$, $y$) dataset. At each iteration (epoch), the loss function is stored and the estimated regression line is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "qnLp_YvzJnmX",
    "outputId": "435b3a6f-fb47-4fbb-bacf-aafeb4f623fc"
   },
   "outputs": [],
   "source": [
    "epochs = 1000 # number of epochs\n",
    "losses = [] # list to stock the loss at each iteration\n",
    "\n",
    "# Loop on epochs\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # compute the prediction using the previous parameters of the neural network\n",
    "    y_pred = linear_regression_model.forward(x)\n",
    "    \n",
    "    # compute and stock the loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # initialize the gradient to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # compute the gradient by back propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the parameter values using the gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    # plot the adjustment at each epoch\n",
    "    plot(x, y_true, 'r')\n",
    "    plot(x, y, 'b.')\n",
    "    plot(x, y_pred.detach().numpy(), 'g')\n",
    "    legend(['Model', 'Data', 'Successive fits'], prop={'size': 20})\n",
    "    title('Linear regression', size=20)\n",
    "    xlabel('x', size=20)\n",
    "    ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4sxgXj8J3rR"
   },
   "source": [
    "It is important to check that the loss function is decreasing. If not, it maybe means that the learning rate in the optimization procedure is too large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "qQkdGcc0J6OL",
    "outputId": "050a6132-db97-4495-edb0-a3e36388eb2c"
   },
   "outputs": [],
   "source": [
    "# Print the loss function\n",
    "plot(range(epochs), losses)\n",
    "title('Loss function', size=20)\n",
    "xlabel('Epoch', size=20)\n",
    "ylabel('Loss value', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZ2s5kTiJ-nx"
   },
   "source": [
    "Finally, let's check if the estimated parameters (at the last epoch) are close to the true ones (i.e., $0.5$ for the slope and $2$ for the bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKI4nQYiJ_gY",
    "outputId": "5eb3f51f-9078-4f58-fec4-6a238bf4df64"
   },
   "outputs": [],
   "source": [
    "# Extract and print the model parameters (weights of the neural network)\n",
    "slope, bias = linear_regression_model.parameters()\n",
    "print('Slope: ', slope)\n",
    "print('Bias: ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAKErwTLKBlR"
   },
   "source": [
    "**Nonlinear regression**\n",
    "\n",
    "Now, we perform a multiple regression between continuous variables $x$ and $y$. Here, we write the nonlinear model $y=2+0.5x-0.05x^2$ and generate $y$ using an additional Gaussian standard random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "rt0s9kKZKENf",
    "outputId": "16c64b37-4105-4d9f-f927-f143db686010"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = torch.randn(1000, 1)*10 # input variable\n",
    "y_true = 2 + 0.5*x - 0.05*x**2 # true model\n",
    "y = y_true + torch.randn(1000, 1)*2 # add noise to the truth\n",
    "\n",
    "# Plot noisy data and true model\n",
    "plot(x, y_true, 'r.')\n",
    "plot(x, y, 'b.')\n",
    "legend(['Model', 'Data'], prop={'size': 20})\n",
    "title('Nonlinear regression', size=20)\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GF5LHH_KIgb"
   },
   "source": [
    "We declare the neural network corresponding to a nonlinear regression. The nonlinearities are introduced by a hidden layer with ReLu activations. Other activation functions are available in PyTorch: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvWqoNJzKLFe"
   },
   "outputs": [],
   "source": [
    "# Declare a class for nonlinear regression\n",
    "class nonlinear_regression_nn(nn.Module):\n",
    "    \n",
    "    # class initialization\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(nonlinear_regression_nn, self).__init__()\n",
    "        # fully connected layer with linear activation\n",
    "        self.fc0 = nn.Linear(input_size, hidden_size)\n",
    "        # ReLu activation\n",
    "        self.relu = nn.ReLU()\n",
    "        # fully connected layer with linear activation\n",
    "        self.fc1 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    # function to apply the neural network\n",
    "    def forward(self, x):\n",
    "        out = self.fc0(x)\n",
    "        out = self.relu(out)\n",
    "        y_pred = self.fc1(out)\n",
    "        return y_pred\n",
    "    \n",
    "# Create the neural network (1 input size for x, 6 neurons in the hidden layer, and 1 output size for y)\n",
    "nonlinear_regression_model = nonlinear_regression_nn(1, 6, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVxxqrkEKQmI"
   },
   "source": [
    "We still use the SGD optimzer and MSE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3qZOfWmKSxU"
   },
   "outputs": [],
   "source": [
    "# Loss function: MSE = sum [(y - y_pred)^2]\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: new_parameters = old_parameters - lr*gradient, with lr the learning rate\n",
    "optimizer = torch.optim.SGD(nonlinear_regression_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNUicMmrKY9X"
   },
   "source": [
    "Now, we fit the neural network using the ($x$, $y$) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1hfn2qJKZqL"
   },
   "outputs": [],
   "source": [
    "epochs = 20000 # number of epochs\n",
    "losses = [] # list to stock the loss at each iteration\n",
    "\n",
    "# Loop on epochs\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # compute the prediction using the previous parameters of the neural network\n",
    "    y_pred = nonlinear_regression_model.forward(x)\n",
    "    \n",
    "    # compute and stock the loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # initialize the gradient to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # compute the gradient by back propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the parameter values using the gradient\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "4gJnBYLDKbrs",
    "outputId": "24f5f29e-897f-418a-f84e-f01dde86e1a4"
   },
   "outputs": [],
   "source": [
    "# Print the loss function\n",
    "plot(range(epochs), losses)\n",
    "title('Loss function', size=20)\n",
    "xlabel('Epoch', size=20)\n",
    "ylabel('Loss value', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "8c6IbnrtKgcB",
    "outputId": "a964bc0f-6b87-41ee-cc29-081069d2980e"
   },
   "outputs": [],
   "source": [
    "# plot the adjustment at the last epoch\n",
    "plot(x, y_true, 'r.')\n",
    "plot(x, y, 'b.')\n",
    "plot(x, y_pred.detach().numpy(), 'g.')\n",
    "legend(['Model', 'Data', 'Fit'], prop={'size': 20})\n",
    "title('Nonlinear regression', size=20)\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5IFLsbjMV5i"
   },
   "source": [
    "What is the generalization performance of the network? (i.e. the accuracy of the prediction for unseen data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "1oqlVx48MsgR",
    "outputId": "91043d6f-5229-4fc9-ac8d-500a17cbbb47"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_RowRU7M4Le"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Introduction_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
